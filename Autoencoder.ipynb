{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization,\n",
        "    UpSampling1D, Reshape, Cropping1D, ZeroPadding1D, Add, LeakyReLU\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.initializers import HeUniform\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, Conv1DTranspose, BatchNormalization, Dropout, Activation,\n",
        "    Add, Flatten, Dense, Reshape\n",
        ")\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vAiLuC_rwpq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Load the filtered signals and split to train, test and validation:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VNBFRihJwSf_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocyjEK9kvJyR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Function to load signals from a folder\n",
        "def load_filtered_signals_with_metadata_from_folder(folder_path, expected_length=5000):\n",
        "    signals_with_metadata = []\n",
        "\n",
        "    # List all .npy files in the folder\n",
        "    file_list = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
        "\n",
        "    for file_name in file_list:\n",
        "        try:\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            signal = np.load(file_path)\n",
        "            first_dim = signal[:, 0] if signal.ndim == 2 else signal\n",
        "\n",
        "            # Filter by length\n",
        "            if len(first_dim) != expected_length:\n",
        "                continue\n",
        "\n",
        "            # Parse ID and lead\n",
        "            file_name_wo_ext = os.path.splitext(file_name)[0]\n",
        "            # file_id, lead = file_name_wo_ext.split('_', 1)\n",
        "            parts = file_name.split('_')\n",
        "            file_id = parts[0]\n",
        "            lead = parts[5]\n",
        "\n",
        "            signals_with_metadata.append((first_dim, file_id, lead))\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {file_name}: {e}\")\n",
        "\n",
        "    return signals_with_metadata\n",
        "\n",
        "# Load the signals\n",
        "data_directory = '/sise/nadav-group/nadavrap-group/ECGs/Final Project/Preprocessing/Filtered_Files_updated/'\n",
        "signals_with_metadata = load_filtered_signals_with_metadata_from_folder(data_directory)\n",
        "\n",
        "print(f\"Total loaded valid signals: {len(signals_with_metadata)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Split into train (70%), validation (10%), and test (20%)\n",
        "train_data, test_data = train_test_split(signals_with_metadata, test_size=0.2, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.125, random_state=42)  # 10% of total for validation\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print(\"Test set size:\", len(test_data))\n",
        "\n",
        "# Optional: convert to arrays ready for Conv1D (if needed later)\n",
        "train_signals = np.array([np.expand_dims(s[0], axis=-1) for s in train_data])\n",
        "val_signals = np.array([np.expand_dims(s[0], axis=-1) for s in val_data])\n",
        "\n",
        "print(f\"train_signals shape: {train_signals.shape}\")\n",
        "print(f\"val_signals shape: {val_signals.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Prepare training data + Normalization according to training signal statistics:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gUH4PfFqw7_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_data(signals_with_metadata):\n",
        "    training_signals = np.array([s[0] for s in signals_with_metadata])\n",
        "    metadata = [(s[1], s[2]) for s in signals_with_metadata]\n",
        "    return training_signals, metadata\n",
        "\n",
        "\n",
        "training_signals, metadata = prepare_training_data(train_data)\n",
        "training_signals = np.expand_dims(training_signals, axis=-1)\n",
        "\n",
        "validation_signals, val_metadata = prepare_training_data(val_data)\n",
        "validation_signals = np.expand_dims(validation_signals, axis=-1)\n",
        "\n",
        "global_mean = np.mean(training_signals)\n",
        "global_std = np.std(training_signals)\n",
        "# Prevent division by zero\n",
        "if global_std == 0:\n",
        "    global_std = 1\n",
        "training_signals = (training_signals - global_mean) / global_std\n",
        "\n",
        "validation_signals = (validation_signals - global_mean) / global_std"
      ],
      "metadata": {
        "id": "fpoX9sb3wqnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***1D Residual CNN Autoencoder Model:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2mWoxz34xWoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, UpSampling1D, BatchNormalization, Dropout,\n",
        "    LeakyReLU, Flatten, Dense, Reshape, Add\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.losses import Huber\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def residual_conv_block(x, filters, kernel_size=7, strides=1, reg=1e-4, dropout_rate=0.2):\n",
        "    shortcut = x\n",
        "    x = Conv1D(filters, kernel_size, strides=strides, padding='same', kernel_regularizer=l2(reg))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv1D(filters, kernel_size, strides=1, padding='same', kernel_regularizer=l2(reg))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Adjust shortcut shape if needed\n",
        "    if shortcut.shape[-1] != filters or strides != 1:\n",
        "        shortcut = Conv1D(filters, 1, strides=strides, padding='same', kernel_regularizer=l2(reg))(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = LeakyReLU()(x)\n",
        "\n",
        "    if dropout_rate > 0:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "def upsample_block(x, filters, kernel_size=7, up_size=2, reg=1e-4):\n",
        "    x = UpSampling1D(size=up_size)(x)\n",
        "    x = Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(reg))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "def build_autoencoder_with_encoder_residuals(input_length=5000, latent_dim=32, reg=1e-4):\n",
        "    input_signal = Input(shape=(input_length, 1))\n",
        "\n",
        "    # Encoder with residual blocks\n",
        "    x = residual_conv_block(input_signal, 32, strides=2, reg=reg)\n",
        "    x = residual_conv_block(x, 64, strides=2, reg=reg)\n",
        "    x = residual_conv_block(x, 128, strides=2, reg=reg)\n",
        "\n",
        "    shape_before_flatten = tf.keras.backend.int_shape(x)[1:]\n",
        "    flat = Flatten()(x)\n",
        "    encoded = Dense(latent_dim, name=\"latent_vector\")(flat)\n",
        "\n",
        "    # Decoder (no residuals)\n",
        "    x = Dense(np.prod(shape_before_flatten))(encoded)\n",
        "    x = Reshape(target_shape=shape_before_flatten)(x)\n",
        "\n",
        "    x = upsample_block(x, 128, up_size=2, reg=reg)\n",
        "    x = upsample_block(x, 64, up_size=2, reg=reg)\n",
        "    x = upsample_block(x, 32, up_size=2, reg=reg)\n",
        "\n",
        "    decoded = Conv1D(1, kernel_size=3, padding='same', activation='linear')(x)\n",
        "\n",
        "    autoencoder = Model(inputs=input_signal, outputs=decoded)\n",
        "    # autoencoder.compile(optimizer=Adam(1e-3), loss=Huber())  # אפשר גם loss='mse'\n",
        "    autoencoder.compile(optimizer=Adam(1e-3), loss='mse')\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "SvimNeFHxlWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_v2 = build_autoencoder_with_encoder_residuals(latent_dim=32)\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
        "\n",
        "history = autoencoder_v2.fit(\n",
        "    training_signals, training_signals,\n",
        "    validation_data=(validation_signals, validation_signals),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "id": "JU9uuQdBxqS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_reconstructed = autoencoder_v2.predict(validation_signals)\n",
        "mse = np.mean((val_reconstructed - validation_signals) ** 2)\n",
        "print(f\"Validation MSE: {mse:.6f}\")"
      ],
      "metadata": {
        "id": "fd_54OxY0r2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_path = '/sise/nadav-group/nadavrap-group/ECGs/Final Project/Autoencoder/autoencode_selin_without_normalization_v2.keras'\n",
        "autoencoder_v2.save(model_path)"
      ],
      "metadata": {
        "id": "ICAeGMCayur9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/sise/nadav-group/nadavrap-group/ECGs/Final Project/Autoencoder/autoencode_selin_without_normalization_v2.keras'\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "loaded_model.summary()"
      ],
      "metadata": {
        "id": "YAicpPIRy2Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Plots:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WapOB0ypyE0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Train MSE')\n",
        "plt.plot(history.history['val_loss'], label='Val MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7QAaZSPmyImS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "for i in range(n):\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    plt.plot(validation_signals[i].squeeze(), label='Original')\n",
        "    plt.plot(val_reconstructed[i].squeeze(), label='Reconstructed', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.title(f\"Signal {i}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PBCm2CY-yJMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Running the model through all the signal data for extracting the latent space:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AKnQwKw42YF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model, Model\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load the model from a SavedModel directory\n",
        "model_path = '/sise/nadav-group/nadavrap-group/ECGs/Final Project/Autoencoder/autoencode_selin_without_normalization_v2.keras'\n",
        "best_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Create a model that outputs the latent space\n",
        "latent_model = Model(inputs=best_model.input, outputs=best_model.get_layer(\"latent_vector\").output)\n",
        "\n",
        "# Directory to save the latent space files\n",
        "latent_space_dir = '/sise/nadav-group/nadavrap-group/ECGs/Final Project/Autoencoder/Latent_Space_without_norm'\n",
        "os.makedirs(latent_space_dir, exist_ok=True)\n",
        "\n",
        "def split_signals_metadata(signals_with_metadata):\n",
        "    \"\"\"\n",
        "    Split a list of tuples into separate arrays for signals and metadata.\n",
        "\n",
        "    Parameters:\n",
        "        signals_with_metadata (list): A list where each item is a tuple:\n",
        "            (signal: np.array, signal_id: str/int, lead: str/int)\n",
        "\n",
        "    Returns:\n",
        "        signals (np.ndarray): Array of signal arrays.\n",
        "        metadata (list of tuples): List of (signal_id, lead) pairs.\n",
        "    \"\"\"\n",
        "    signals = np.array([s[0] for s in signals_with_metadata])\n",
        "    metadata = [(s[1], s[2]) for s in signals_with_metadata]\n",
        "    return signals, metadata\n",
        "\n",
        "def save_latent_space_with_examples(signals, metadata):\n",
        "    \"\"\"\n",
        "    Extract and save latent space vectors from the given signals using the autoencoder.\n",
        "\n",
        "    If a file with the same name already exists, saves the new file with '_extra' appended to the filename\n",
        "    to avoid overwriting existing files.\n",
        "\n",
        "    Parameters:\n",
        "        signals (np.ndarray): Array of input signals to encode.\n",
        "        metadata (list of tuples): Metadata for each signal (signal_id, lead).\n",
        "    \"\"\"\n",
        "    example_count = 0\n",
        "    for i, signal in enumerate(signals):\n",
        "        # Extract latent vector\n",
        "        latent_vector = latent_model.predict(np.expand_dims(signal, axis=0))[0]\n",
        "\n",
        "        # Construct filename from metadata\n",
        "        signal_id, lead = metadata[i]\n",
        "        filename = f\"{signal_id}_{lead}.npy\"\n",
        "        filepath = os.path.join(latent_space_dir, filename)\n",
        "\n",
        "        # If file exists, add '_extra' suffix to filename\n",
        "        if os.path.exists(filepath):\n",
        "            filename = f\"{signal_id}_{lead}_extra.npy\"\n",
        "            filepath = os.path.join(latent_space_dir, filename)\n",
        "\n",
        "        # Save latent vector as .npy\n",
        "        np.save(filepath, latent_vector)\n",
        "\n",
        "        # Print first 3 examples\n",
        "        if example_count < 3:\n",
        "            print(f\"Example {example_count + 1}:\")\n",
        "            print(f\"Metadata: ID={signal_id}, Lead={lead}\")\n",
        "            print(f\"Latent space length: {len(latent_vector)}\")\n",
        "            print(f\"Latent vector: {latent_vector}\\n\")\n",
        "            example_count += 1\n",
        "\n",
        "        print(f\"Saved latent space for signal {i+1}/{len(signals)} to {filepath}\")\n",
        "\n",
        "# ---- Usage Example ----\n",
        "\n",
        "all_signals, metadata = prepare_training_data(signals_with_metadata)\n",
        "all_signals = np.expand_dims(all_signals, axis=-1)\n",
        "all_signals = (all_signals - global_mean) / global_std\n",
        "# Assuming `signals_with_metadata` is a list of (signal, id, lead) tuples\n",
        "# signals, metadata = split_signals_metadata(signals_with_metadata)\n",
        "# norm_signals = normalize_signals(signals)  # Assuming this function is defined elsewhere\n",
        "save_latent_space_with_examples(all_signals, metadata)"
      ],
      "metadata": {
        "id": "2yJTQnvk2OiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Filtering the latent space files acoording MSE < 0.4:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "g38_eNA80uVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "\n",
        "# חיזוי על כל הדגימות\n",
        "reconstructed = best_model.predict(all_signals)\n",
        "\n",
        "# חישוב MSE לכל דגימה\n",
        "mse_per_sample = np.mean((reconstructed - all_signals) ** 2, axis=(1, 2))  # axis 1,2 = לאורך הזמן ולערוצים\n",
        "\n",
        "# יצירת טבלה עם mse ונתוני metadata תואמים\n",
        "df = pd.DataFrame({\n",
        "    'mse': mse_per_sample,\n",
        "    'file_id': [meta[0] for meta in metadata],\n",
        "    'lead': [meta[1] for meta in metadata]\n",
        "})\n",
        "\n",
        "# מיון לפי MSE מהנמוך לגבוה\n",
        "df_sorted = df.sort_values(by='mse').reset_index(drop=True)\n",
        "df_sorted"
      ],
      "metadata": {
        "id": "rh1eynckyh1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "low_mse_df = df_sorted[df_sorted['mse'] < 0.4].reset_index(drop=True)\n",
        "low_mse_df"
      ],
      "metadata": {
        "id": "wtiWTFw305EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Input: sorted dataframe with low MSE entries\n",
        "# Assume low_mse_df is already defined\n",
        "# low_mse_df = df_sorted[df_sorted['mse'] < 0.3].reset_index(drop=True)\n",
        "\n",
        "latent_dir = '/sise/nadav-group/nadavrap-group/ECGs/Final Project/Autoencoder/Latent_Space_without_norm'\n",
        "dest_dir = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/Autoencoder/Filtered_Low_MSE_without_norm\"\n",
        "\n",
        "# Create destination directory if it doesn't exist\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Track how many files we actually copy\n",
        "copied_count = 0\n",
        "not_found = []\n",
        "\n",
        "# Loop through each row in the filtered dataframe\n",
        "for _, row in low_mse_df.iterrows():\n",
        "    file_id = str(row['file_id'])\n",
        "    lead = str(row['lead'])\n",
        "\n",
        "    filename = f\"{file_id}_{lead}.npy\"\n",
        "    src_path = os.path.join(latent_dir, filename)\n",
        "    dst_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.copy2(src_path, dst_path)\n",
        "        copied_count += 1\n",
        "    else:\n",
        "        not_found.append(filename)\n",
        "\n",
        "# Report\n",
        "print(\"✅ Finished copying.\")\n",
        "print(f\"Files copied: {copied_count}\")\n",
        "print(f\"Missing files (not found): {len(not_found)}\")\n",
        "\n",
        "if not_found:\n",
        "    print(\"Example missing files:\")\n",
        "    for f in not_found[:5]:\n",
        "        print(f)"
      ],
      "metadata": {
        "id": "62NircwW25Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Creating ECG Phenotype txt files:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9WDXx0yN3rqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "input_folder = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/Autoencoder/Filtered_Low_MSE_without_norm\"\n",
        "output_folder = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/GWAS/ECG_PHENOTYPE_without_norm\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "lead_to_data = defaultdict(list)\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.npy') and 'extra' not in filename:\n",
        "        parts = filename.replace('.npy', '').split('_')\n",
        "        if len(parts) != 2:\n",
        "            continue\n",
        "        file_id, lead = parts\n",
        "        vec_path = os.path.join(input_folder, filename)\n",
        "        try:\n",
        "            vec = np.load(vec_path)\n",
        "            lead_to_data[lead].append((file_id, vec))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {filename}: {e}\")\n",
        "\n",
        "for lead, samples in lead_to_data.items():\n",
        "    output_file = os.path.join(output_folder, f\"ecg_phenotype_{lead}.txt\")\n",
        "    with open(output_file, 'w') as f:\n",
        "        # Writing Headings\n",
        "        header = ['IID'] + [f'feature_{i+1}' for i in range(len(samples[0][1]))]\n",
        "        f.write('\\t'.join(header) + '\\n')\n",
        "\n",
        "        # Writing Data\n",
        "        for file_id, vec in samples:\n",
        "            line = [file_id] + [f'{float(v):.8f}' for v in vec]\n",
        "            f.write('\\t'.join(line) + '\\n')"
      ],
      "metadata": {
        "id": "ejbjZCQ33sJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Plots:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W4Lh_QMt4DMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df_sorted.index, df_sorted['mse'], marker='o', linestyle='-')\n",
        "plt.title(\"MSE per Sample (Sorted)\")\n",
        "plt.xlabel(\"Sample Index (Sorted by MSE)\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GlWAXg2d4Cx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df_sorted['mse'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Histogram of MSE Values per Sample\")\n",
        "plt.xlabel(\"MSE\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4HKM0h0O5Cji"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}