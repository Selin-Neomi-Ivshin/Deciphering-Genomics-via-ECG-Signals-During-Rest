{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Imports:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YxKBir6Tlf7C"
      },
      "id": "YxKBir6Tlf7C"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob"
      ],
      "metadata": {
        "id": "JhmsaSZ1lmqo"
      },
      "id": "JhmsaSZ1lmqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***🧬 Merging Significant SNPs from Multiple Folders:***\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FxHnmFerlrYT"
      },
      "id": "FxHnmFerlrYT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa22483",
      "metadata": {
        "id": "7aa22483",
        "outputId": "6b89a2d1-7346-461d-a5ac-569b9c4dc571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Combined CSV saved to: /sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/ALL_significant_snps_combined.csv\n"
          ]
        }
      ],
      "source": [
        "# Base directory containing all phenotype folders\n",
        "base_dir = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/\"\n",
        "\n",
        "# List all phenotype directories within the base directory\n",
        "phenotype_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
        "\n",
        "# List to store all the individual dataframes\n",
        "all_dataframes = []\n",
        "\n",
        "# Loop through each phenotype directory\n",
        "for pheno in phenotype_dirs:\n",
        "    pheno_path = os.path.join(base_dir, pheno)\n",
        "\n",
        "    # List all feature directories within the current phenotype directory\n",
        "    feature_dirs = [d for d in os.listdir(pheno_path) if os.path.isdir(os.path.join(pheno_path, d))]\n",
        "\n",
        "\n",
        "    # Loop through each feature directory\n",
        "    for feat in feature_dirs:\n",
        "        feat_path = os.path.join(pheno_path, feat)\n",
        "\n",
        "        # Find all CSV files matching the pattern (significant SNPs per chromosome-feature)\n",
        "        csv_files = glob.glob(os.path.join(feat_path, \"significant_snps_chrom*_feature*.csv\"))\n",
        "\n",
        "        # Loop through the matched CSV files\n",
        "        for csv_file in csv_files:\n",
        "            try:\n",
        "                # Read the CSV file into a DataFrame\n",
        "                df = pd.read_csv(csv_file)\n",
        "                # Add metadata columns: phenotype name, feature name, and original filename\n",
        "                df[\"phenotype\"] = pheno\n",
        "                df[\"feature\"] = feat\n",
        "                df[\"source_file\"] = os.path.basename(csv_file)\n",
        "\n",
        "                # Append to the list of dataframes\n",
        "                all_dataframes.append(df)\n",
        "            except Exception as e:\n",
        "                # If an error occurs during reading, print the error message\n",
        "                print(f\"❌ Error reading {csv_file}: {e}\")\n",
        "\n",
        "# If any dataframes were collected, concatenate and save to a combined CSV\n",
        "if all_dataframes:\n",
        "    final_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "    output_path = os.path.join(base_dir, \"ALL_significant_snps_combined.csv\")\n",
        "    final_df.to_csv(output_path, index=False)\n",
        "    print(f\"✅ Combined CSV saved to: {output_path}\")\n",
        "else:\n",
        "    # If no CSV files were found, print a warning\n",
        "    print(\"⚠️ No CSV files found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Extract Full ID column for Haploreg:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0xFee4_9nEGL"
      },
      "id": "0xFee4_9nEGL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003a52f2",
      "metadata": {
        "id": "003a52f2",
        "outputId": "257ecaaf-878e-4a8e-bfce-b3e383c09669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ID list saved to: /sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/significant_IDs_list.txt\n"
          ]
        }
      ],
      "source": [
        "# Path to the combined CSV file\n",
        "csv_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/ALL_significant_snps_combined.csv\"\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Check if the 'ID' column exists in the DataFrame\n",
        "if \"ID\" not in df.columns:\n",
        "    raise ValueError(\"❌ Column 'ID' not found in the CSV file. Please check the column name.\")\n",
        "\n",
        "# Extract all non-NaN values from the 'ID' column, convert to string, and create a list\n",
        "id_values = df[\"ID\"].dropna().astype(str).tolist()\n",
        "# Join all ID values into a single comma-separated string\n",
        "id_string = \",\".join(id_values)\n",
        "\n",
        "# Define the output path for the text file (same directory as the CSV)\n",
        "output_txt_path = os.path.join(os.path.dirname(csv_path), \"significant_IDs_list.txt\")\n",
        "# Write the ID string to the output text file\n",
        "with open(output_txt_path, \"w\") as f:\n",
        "    f.write(id_string)\n",
        "# Confirmation message\n",
        "print(f\"✅ ID list saved to: {output_txt_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Extract Unique IDs for Haploreg:***\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "e0Paf_tlnPUi"
      },
      "id": "e0Paf_tlnPUi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b22587e",
      "metadata": {
        "id": "0b22587e",
        "outputId": "31573be8-b108-47cf-b59d-39b7e5f0e1fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ סיום. הקובץ החדש שמור ב: /sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/unique_significant_IDs.txt\n"
          ]
        }
      ],
      "source": [
        "# Path to the original text file containing comma-separated IDs\n",
        "input_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/significant_IDs_list.txt\"\n",
        "\n",
        "# Path to save the cleaned file after removing duplicates\n",
        "output_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/unique_significant_IDs.txt\"\n",
        "\n",
        "# Read the entire content of the input file\n",
        "with open(input_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Split the string by commas and remove any extra whitespace from each ID\n",
        "ids = [x.strip() for x in text.split(\",\")]\n",
        "\n",
        "# Remove duplicate IDs using set, then sort them alphabetically\n",
        "unique_ids = sorted(set(ids))\n",
        "\n",
        "# Join the unique IDs back into a single comma-separated string\n",
        "cleaned_text = \",\".join(unique_ids)\n",
        "\n",
        "# Write the cleaned string to a new output file\n",
        "with open(output_path, \"w\") as f:\n",
        "    f.write(cleaned_text)\n",
        "\n",
        "# Print confirmation message\n",
        "print(\"✅ Done. The cleaned file has been saved to:\", output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Unique IDs without IDs of type Affx:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "o_i9cnncnhyQ"
      },
      "id": "o_i9cnncnhyQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4467e25b",
      "metadata": {
        "id": "4467e25b",
        "outputId": "cec5c97a-e7e8-46c2-e5c1-08ffa6fbbc5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ סיום. הקובץ ללא מזהי Affx נוצר ונשמר בשם:\n",
            "/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/unique_no_affx_IDs.txt\n"
          ]
        }
      ],
      "source": [
        "# Input file that contains a list of unique SNP identifiers\n",
        "input_unique_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/unique_significant_IDs.txt\"\n",
        "\n",
        "# Output file to save the filtered IDs (excluding those that start with \"Affx\")\n",
        "output_no_affx_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/unique_no_affx_IDs.txt\"\n",
        "\n",
        "# Read the content of the input file\n",
        "with open(input_unique_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Split the string by commas and remove extra whitespace\n",
        "ids = [x.strip() for x in text.split(\",\")]\n",
        "# Filter out any IDs that start with \"Affx\" (case-insensitive) and sort the remaining IDs\n",
        "filtered_ids = sorted([id_ for id_ in ids if not id_.lower().startswith(\"affx\")])\n",
        "\n",
        "# Write the cleaned, filtered IDs to the new output file as a comma-separated string\n",
        "with open(output_no_affx_path, \"w\") as f:\n",
        "    f.write(\",\".join(filtered_ids))\n",
        "\n",
        "# Print confirmation messages\n",
        "print(\"✅ Done. The file without Affx IDs has been created and saved as:\")\n",
        "print(output_no_affx_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae76a38",
      "metadata": {
        "id": "4ae76a38",
        "outputId": "bb73edf7-664e-4446-c798-953bce3874e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔢 מספר המזהים בקובץ: 675699\n"
          ]
        }
      ],
      "source": [
        "# Path to the file containing the filtered, unique SNP IDs\n",
        "file_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/unique_no_affx_IDs.txt\"\n",
        "\n",
        "\n",
        "# Read the content of the file\n",
        "with open(file_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Split the text by commas, strip whitespace, and ignore empty entries\n",
        "ids = [x.strip() for x in text.split(\",\") if x.strip()]\n",
        "\n",
        "# Count the number of valid IDs\n",
        "num_ids = len(ids)\n",
        "\n",
        "# Print the number of IDs\n",
        "print(f\"🔢 Number of IDs in the file: {num_ids}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69481cc0",
      "metadata": {
        "id": "69481cc0",
        "outputId": "289b1cf4-a8d2-454c-82e4-a1554028cdcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ קבצים נוצרו בהצלחה:\n",
            "🔹 CSV: /sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/top_1000_unique_snps.csv\n",
            "🔹 TXT: /sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/top_1000_ids.txt\n"
          ]
        }
      ],
      "source": [
        "# Path to the original CSV file containing all significant SNPs\n",
        "csv_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/ALL_significant_snps_combined.csv\"\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Sort the DataFrame in descending order based on the 17th column (index 16), assumed to be -LOG10(P)\n",
        "df_sorted = df.sort_values(by=df.columns[16], ascending=False)\n",
        "\n",
        "# Remove duplicate rows based on the 'ID' column (keep only the first occurrence)\n",
        "df_unique = df_sorted.drop_duplicates(subset='ID')\n",
        "\n",
        "# Filter out rows where the 'ID' starts with 'Affx' (case-insensitive)\n",
        "df_filtered = df_unique[~df_unique['ID'].str.lower().str.startswith('affx')]\n",
        "\n",
        "# Select the top 1000 rows after filtering and deduplication\n",
        "top_1000 = df_filtered.head(1000)\n",
        "\n",
        "# Save the top 1000 SNPs to a new CSV file\n",
        "output_csv_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/top_1000_unique_snps.csv\"\n",
        "top_1000.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# Extract the list of SNP IDs from the top 1000 and join into a single comma-separated string\n",
        "unique_ids = top_1000['ID'].tolist()\n",
        "ids_text = \",\".join(unique_ids)\n",
        "\n",
        "# Save the list of IDs to a plain text file\n",
        "output_txt_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/top_1000_ids.txt\"\n",
        "with open(output_txt_path, \"w\") as f:\n",
        "    f.write(ids_text)\n",
        "\n",
        "# Print success messages with paths to the output files\n",
        "print(\"✅ Files created successfully:\")\n",
        "print(\"🔹 CSV:\", output_csv_path)\n",
        "print(\"🔹 TXT:\", output_txt_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Extract top 1000 IDs:***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jUq2GFczpUOz"
      },
      "id": "jUq2GFczpUOz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849f8401",
      "metadata": {
        "id": "849f8401",
        "outputId": "a1603f1f-6b7d-43e0-db05-43e79205af69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ הקובץ עם רשימת ה־IDs נוצר בהצלחה:\n",
            "/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/top_1000_ids.txt\n"
          ]
        }
      ],
      "source": [
        "# Path to the input CSV file containing the top 1000 SNPs\n",
        "input_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/top_1000_unique_snps.csv\"\n",
        "\n",
        "# Path to the output text file where the list of IDs will be saved\n",
        "output_txt_path = \"/sise/nadav-group/nadavrap-group/ECGs/Final Project/P-VALUE/Manhattan_plots_without_norm/top_1000_ids.txt\"\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Extract the 'ID' column as a list (assuming no duplicates need to be removed again)\n",
        "unique_ids = df['ID'].tolist()\n",
        "\n",
        "# Join the IDs into a single comma-separated string\n",
        "ids_text = \",\".join(unique_ids)\n",
        "\n",
        "# Print confirmation message\n",
        "with open(output_txt_path, \"w\") as f:\n",
        "    f.write(ids_text)\n",
        "\n",
        "print(\"✅ הקובץ עם רשימת ה־IDs נוצר בהצלחה:\")\n",
        "print(output_txt_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}